# MVPBENCH

This repo contains code evaluation and dataset for the paper MVPBench: A Multi-Video Perception Evaluation Benchmark for Multi-modal Video Understanding

## Introduction
The rapid progress of Large Language Models (LLMs) has spurred growing interest in Multi-modal LLMs (MLLMs) and motivated the development of benchmarks to evaluate their perceptual and comprehension abilities. Existing benchmarks, however, are limited to static images or single videos, overlooking the complex interactions across multiple videos. To address this gap, we introduce the **M**ulti-**V**ideo **P**erception Evaluation **Bench**mark (**MVPBench**), a new benchmark featuring **14** subtasks across diverse visual domains that task models with extracting relevant information from video sequences to make informed decisions. MVPBench includes **1K** question-answering tests involving **2.7K** video clips sourced from existing datasets and manually annotated clips. Extensive evaluations reveal that current models struggle to process multi-video inputs effectively, underscoring substantial limitations in their multi-video comprehension. We anticipate MVPBench will drive advancements in multi-video perception.
